{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepare_opennmt_bulgarian_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWUjt_E8TxZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://nlp.ffzg.hr/data/corpora/setimes/setimes.bg-en.txt.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v8UqSdBLM5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xzvf setimes.bg-en.txt.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6QF4JcOT4gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm, trange\n",
        "from nltk import tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "en_data = []\n",
        "bg_data = []\n",
        "  \n",
        "\n",
        "with open('/content/setimes.bg-en.txt/setimes.bg-en.bg.txt') as bg, open('/content/setimes.bg-en.txt/setimes.bg-en.en.txt') as en:\n",
        "    for bg_line, en_line in zip(bg, en):\n",
        "        en_data.append(en_line.rstrip() + '\\n')\n",
        "        bg_data.append(bg_line.rstrip() + '\\n')\n",
        "\n",
        "\n",
        "total_test = 40000\n",
        "en_train, en_subtotal, bg_train, bg_subtotal = train_test_split(\n",
        "        en_data, bg_data, test_size=total_test, random_state=42)\n",
        "\n",
        "en_test, en_val, bg_test, bg_val = train_test_split(\n",
        "        en_subtotal, bg_subtotal, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "with open(\"en_train.txt\", \"w\") as f:\n",
        "    f.write(\n",
        "        \"\\n\".join([\n",
        "             \" \".join(tokenize.word_tokenize(l))\n",
        "             for l in en_train\n",
        "         ])\n",
        "    )\n",
        "\n",
        "with open(\"bg_train.txt\", \"w\") as f:\n",
        "    f.write(\n",
        "        \"\\n\".join([\n",
        "             \" \".join(tokenize.word_tokenize(l))\n",
        "             for l in bg_train\n",
        "         ])\n",
        "    )\n",
        "with open(\"en_val.txt\", \"w\") as f:\n",
        "    f.write(\n",
        "        \"\\n\".join([\n",
        "             \" \".join(tokenize.word_tokenize(l))\n",
        "             for l in en_val\n",
        "         ])\n",
        "    )\n",
        "\n",
        "with open(\"bg_val.txt\", \"w\") as f:\n",
        "    f.write(\n",
        "        \"\\n\".join([\n",
        "             \" \".join(tokenize.word_tokenize(l))\n",
        "             for l in bg_val\n",
        "         ])\n",
        "    )\n",
        "\n",
        "with open(\"en_test.txt\", \"w\") as f:\n",
        "    f.write(\n",
        "        \"\\n\".join([\n",
        "             \" \".join(tokenize.word_tokenize(l))\n",
        "             for l in en_test\n",
        "         ])\n",
        "    )\n",
        "\n",
        "with open(\"bg_test.txt\", \"w\") as f:\n",
        "    f.write(\n",
        "        \"\\n\".join([\n",
        "             \" \".join(tokenize.word_tokenize(l))\n",
        "             for l in bg_test\n",
        "         ])\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}